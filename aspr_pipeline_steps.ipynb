{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\r\n",
        "from azureml.core import Workspace\r\n",
        "\r\n",
        "# Load the workspace from the saved config file\r\n",
        "ws = Workspace.from_config()\r\n",
        "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))\r\n",
        "# ws.get_details()\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Ready to use Azure ML 1.42.0 to work with tcp0044aedcmluc001dtmlws\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1659951702452
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "cluster_name = \"tcp0044uc001cc03\"\r\n",
        "\r\n",
        "try:\r\n",
        "    # Check for existing compute target\r\n",
        "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
        "    print('Found existing cluster, use it.')\r\n",
        "except ComputeTargetException:\r\n",
        "    # If it doesn't already exist, create it\r\n",
        "    try:\r\n",
        "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\r\n",
        "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
        "        pipeline_cluster.wait_for_completion(show_output=True)\r\n",
        "    except Exception as ex:\r\n",
        "        print(ex)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found existing cluster, use it.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1659951702643
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "# Create a folder for the pipeline step files\r\n",
        "experiment_folder = 'ASPR_pipeline'\r\n",
        "os.makedirs(experiment_folder, exist_ok=True)\r\n",
        "\r\n",
        "print(experiment_folder)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "ASPR_pipeline\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1659951702819
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/python_env2.yml\r\n",
        "\r\n",
        "name: python_env2\r\n",
        "\r\n",
        "dependencies:\r\n",
        "- python\r\n",
        "- anaconda\r\n",
        "- pip\r\n",
        "- pip:\r\n",
        "      - azureml.sdk\r\n",
        "      - sklearn\r\n",
        "      - statsmodels\r\n",
        "      - pyarrow\r\n",
        "      - numpy\r\n",
        "      - pandas\r\n",
        "      - seaborn\r\n",
        "      - joblib\r\n",
        "      - shap\r\n",
        "      - graphviz \r\n",
        "      - tensorflow\r\n",
        "      - plotly\r\n",
        "      - pmdarima\r\n",
        "      - emmv\r\n",
        "           "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ASPR_pipeline/python_env2.yml\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/aspr_script.py\r\n",
        "#!/usr/bin/env python\r\n",
        "# coding: utf-8\r\n",
        "\r\n",
        "from __future__ import print_function\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import sklearn\r\n",
        "from sklearn.covariance import EllipticEnvelope\r\n",
        "import plotly.express as px\r\n",
        "from sklearn.ensemble import IsolationForest\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.feature_selection import RFE\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import statsmodels.api as sm\r\n",
        "#import plotly.graph_objects as go\r\n",
        "#%matplotlib inline\r\n",
        "from sklearn import linear_model\r\n",
        "from sklearn.linear_model import LinearRegression\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.linear_model import Ridge\r\n",
        "from sklearn.linear_model import Lasso\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "#from emmv import emmv_scores\r\n",
        "\r\n",
        "# Supress Warnings\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "from sklearn.metrics import precision_score\r\n",
        "from sklearn.metrics import recall_score\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "import os\r\n",
        "from datetime import date\r\n",
        "import re\r\n",
        "from statsmodels.tsa.stattools import adfuller\r\n",
        "from statsmodels.tsa.arima_model import ARIMA\r\n",
        "\r\n",
        "from statsmodels.tsa.arima_process import ArmaProcess\r\n",
        "from statsmodels.tsa.arima_process import ArmaProcess\r\n",
        "from statsmodels.tsa.arima_model import ARMA\r\n",
        "#from statsmodelstsaarima_model\r\n",
        "from joblib import Parallel, delayed\r\n",
        "from pmdarima.arima.utils import ndiffs\r\n",
        "from statsmodels.tsa.api import VAR\r\n",
        "import joblib\r\n",
        "import sklearn\r\n",
        "from azureml.core import Model\r\n",
        "from azureml.core.resource_configuration import ResourceConfiguration\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.covariance import EllipticEnvelope\r\n",
        "import plotly.express as px\r\n",
        "from sklearn.ensemble import IsolationForest\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import statsmodels.api as sm\r\n",
        "import plotly.graph_objects as go\r\n",
        "from sklearn import linear_model\r\n",
        "from sklearn.linear_model import LinearRegression\r\n",
        "from sklearn.linear_model import Ridge\r\n",
        "from sklearn.linear_model import Lasso\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.covariance import EllipticEnvelope\r\n",
        "#%matplotlib inline\r\n",
        "#from sklearn.tree import DecisionTreeClassifier, export_graphviz\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import shap \r\n",
        "#import graphviz\r\n",
        "sns.set_style('darkgrid')\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\r\n",
        "#import chart_studio.plotly as py\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from matplotlib import pyplot\r\n",
        "import plotly.graph_objs as go\r\n",
        "from sklearn.svm import OneClassSVM\r\n",
        "from numpy import where\r\n",
        "init_notebook_mode(connected=True)\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "#import theano\r\n",
        "#import theano.tensor as T\r\n",
        "\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "#from emmv import emmv_scores\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Activation, InputLayer\r\n",
        "from tensorflow.keras.preprocessing import image\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\r\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D\r\n",
        "#from keras.utils import np_utils\r\n",
        "from tensorflow.keras.preprocessing import sequence\r\n",
        "#from tensorflow.keras.layers.embeddings import Embedding\r\n",
        "#from tensorflow.keras.layers.recurrent import SimpleRNN\r\n",
        "#from sklearn.cross_validation import train_test_split\r\n",
        "#from tensorflow.keras.layers.core import Activation, RepeatVector\r\n",
        "#from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
        "#from tensorflow.keras.layers import TimeDistributed\r\n",
        "# azureml-core of version 1.0.72 or higher is required\r\n",
        "# azureml-dataprep[pandas] of version 1.1.34 or higher is required\r\n",
        "from azureml.core import Workspace, Dataset\r\n",
        "from azureml.core.run import Run\r\n",
        "\r\n",
        "# subscription_id = 'c4f0bb45-07a9-489a-8f6a-6fbc64f25b46'\r\n",
        "# resource_group = 'tcp-azu0044-ae-rg-dcml-uc001-dt'\r\n",
        "# workspace_name = 'tcp0044aedcmluc001dtmlws'\r\n",
        "\r\n",
        "# workspace = Workspace(subscription_id, resource_group, workspace_name)\r\n",
        "\r\n",
        "workspace = Run.get_context().experiment.workspace\r\n",
        "run = Run.get_context()\r\n",
        "ws = run.experiment.workspace\r\n",
        "\r\n",
        "ASPR_data = Dataset.get_by_name(workspace, name='Cycletime_2022')\r\n",
        "ASPR_data = ASPR_data.to_pandas_dataframe()\r\n",
        "\r\n",
        "ASPR_data_new = ASPR_data.loc[: ,['OrderN','CycleTim','BusinessUnit','OrderTypeKey','TechType','InstallationTypeKey','AgeRangeKey','State','IndustryPartnerName','CustomerTypeKey','OrderChannel','NBNServiceClass','Diff_SIK_HardwareTCD','Stage1_Cycletime','Stage2_Cycletime','Stage3_Cycletime','Stage4_Cycletime','FieldActivityStateKey','PartialCompleteCntKey','FieldActivityRescheduleStateKey','RescheduleDriver','RescheduleCntKey','ItamNumber','SubmitToResolve']]\r\n",
        "\r\n",
        "ASPR_data_new['State'] = ASPR_data_new['State'].fillna('NA')\r\n",
        "ASPR_data_new['OrderChannel'] = ASPR_data_new['OrderChannel'].fillna('NA')\r\n",
        "ASPR_data_new['NBNServiceClass'] = ASPR_data_new['NBNServiceClass'].fillna('NA')\r\n",
        "ASPR_data_new['Diff_SIK_HardwareTCD'] = ASPR_data_new['Diff_SIK_HardwareTCD'].fillna(0)\r\n",
        "ASPR_data_new['Stage1_Cycletime'] = ASPR_data_new['Stage1_Cycletime'].fillna(0)\r\n",
        "ASPR_data_new['Stage2_Cycletime'] = ASPR_data_new['Stage2_Cycletime'].fillna(0)\r\n",
        "ASPR_data_new['Stage3_Cycletime'] = ASPR_data_new['Stage3_Cycletime'].fillna(0)\r\n",
        "ASPR_data_new['Stage4_Cycletime'] = ASPR_data_new['Stage4_Cycletime'].fillna(0)\r\n",
        "ASPR_data_new['FieldActivityStateKey'] = ASPR_data_new['FieldActivityStateKey'].fillna('NA')\r\n",
        "ASPR_data_new['PartialCompleteCntKey'] = ASPR_data_new['PartialCompleteCntKey'].fillna(0)\r\n",
        "ASPR_data_new['FieldActivityRescheduleStateKey'] = ASPR_data_new['FieldActivityRescheduleStateKey'].fillna('NA')\r\n",
        "ASPR_data_new['RescheduleDriver'] = ASPR_data_new['RescheduleDriver'].fillna('NA')\r\n",
        "ASPR_data_new['RescheduleCntKey'] = ASPR_data_new['RescheduleCntKey'].fillna(0)\r\n",
        "ASPR_data_new['ItamNumber'] = ASPR_data_new['ItamNumber'].fillna(0)\r\n",
        "ASPR_data_new['SubmitToResolve'] = ASPR_data_new['SubmitToResolve'].fillna(0)\r\n",
        "\r\n",
        "def condition(x):\r\n",
        "    if x>0:\r\n",
        "        return \"Customer delay\"\r\n",
        "    elif x<0:\r\n",
        "        return \"Telstra delay\"\r\n",
        "    else:\r\n",
        "        return \"NA\"\r\n",
        "\r\n",
        "ASPR_data_new['Diff_SIK_HardwareTCD'] = ASPR_data_new['Diff_SIK_HardwareTCD'].apply(condition)\r\n",
        "\r\n",
        "ASPR_data_new['BU_Ordertype'] = ASPR_data_new['BusinessUnit']+ '-' +ASPR_data_new['OrderTypeKey']\r\n",
        "\r\n",
        "ASPR_data_new=ASPR_data_new.drop(['BusinessUnit','OrderTypeKey'], axis=1)\r\n",
        "\r\n",
        "ASPR_data_new1 = pd.get_dummies(ASPR_data_new,columns = ['TechType', 'InstallationTypeKey', 'AgeRangeKey', 'State','IndustryPartnerName', 'CustomerTypeKey', 'OrderChannel','NBNServiceClass','Diff_SIK_HardwareTCD', 'FieldActivityStateKey', 'FieldActivityRescheduleStateKey', 'RescheduleDriver'])\r\n",
        "\r\n",
        "def drop_allzero_columns(df):\r\n",
        "    for i in df.columns:\r\n",
        "        if ((df[i] == 0.0).all() == True ):\r\n",
        "            df.drop([i], inplace=True, axis=1)\r\n",
        "    return df\r\n",
        "\r\n",
        "df_final = drop_allzero_columns(ASPR_data_new1)\r\n",
        "\r\n",
        "\r\n",
        "# # Testing Causation using Granger’s Causality Test\r\n",
        "\r\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\r\n",
        "maxlag=12\r\n",
        "test = 'ssr_chi2test'\r\n",
        "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \r\n",
        "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\r\n",
        "    The rows are the response variable, columns are predictors. The values in the table \r\n",
        "    are the P-Values. P-Values lesser than the significance level (0.05), implies \r\n",
        "    the Null Hypothesis that the coefficients of the corresponding past values is \r\n",
        "    zero, that is, the X does not cause Y can be rejected.\r\n",
        "\r\n",
        "    data      : pandas dataframe containing the time series variables\r\n",
        "    variables : list containing names of the time series variables.\r\n",
        "    \"\"\"\r\n",
        "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\r\n",
        "    for c in df.columns:\r\n",
        "        for r in df.index:\r\n",
        "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\r\n",
        "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\r\n",
        "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\r\n",
        "            min_p_value = np.min(p_values)\r\n",
        "            df.loc[r, c] = min_p_value\r\n",
        "    df.columns = [var + '_x' for var in variables]\r\n",
        "    df.index = [var + '_y' for var in variables]\r\n",
        "    return df\r\n",
        "\r\n",
        "\r\n",
        "df_final['CycleTim'] = pd.to_numeric(df_final['CycleTim'],errors='coerce').fillna(0)\r\n",
        "\r\n",
        "df_final = df_final.rename(columns = {'CycleTim':'Target_var','BU_Ordertype':'Category'}, inplace = False)\r\n",
        "\r\n",
        "df_final_numericcolumns = df_final.select_dtypes(include =['float64','int64','uint8'])\r\n",
        "\r\n",
        "correlation= df_final_numericcolumns.corr(method='pearson')\r\n",
        "\r\n",
        "sorted_mat = correlation.unstack().sort_values()\r\n",
        "\r\n",
        "batch_folder = 'Output'\r\n",
        "os.makedirs(batch_folder, exist_ok=True)\r\n",
        "\r\n",
        "# Extract all the files to csv\r\n",
        "local_path1 = 'Output/asprcorrelation.csv'\r\n",
        "correlation.to_csv(local_path1)\r\n",
        "\r\n",
        "variables1 = df_final_numericcolumns.columns\r\n",
        "\r\n",
        "\r\n",
        "# X causing Y. If less than significance level, its good. for eg less than 0.05\r\n",
        "\r\n",
        "#df_final_causality = grangers_causation_matrix(df_final, variables = variables1)\r\n",
        "\r\n",
        "# Extract all the files to csv\r\n",
        "#local_path2 = 'Output/df_final_causality.csv'\r\n",
        "#df_final_causality.to_csv(local_path2)\r\n",
        "\r\n",
        "#df_causality = df_final_causality.loc[df_final_causality.index == 'Target_var_y']\r\n",
        "\r\n",
        "#def drop_lesscausality(df):\r\n",
        "    #droppedcols=[]\r\n",
        "#    for i in df.columns:\r\n",
        "#        if (df[i][0] > 0.05):\r\n",
        "#            df.drop([i], inplace=True, axis=1)\r\n",
        "#            #droppedcols = droppedcols+i\r\n",
        "#    return df\r\n",
        "\r\n",
        "#df1 = drop_lesscausality(df_causality)\r\n",
        "\r\n",
        "#k = []\r\n",
        "#def drop_causality(df):\r\n",
        "#    for i in df.columns:\r\n",
        "#        j=i[:-2]\r\n",
        "#        k.append(j)\r\n",
        "#    return k\r\n",
        "\r\n",
        "\r\n",
        "#list1 = drop_causality(df1)\r\n",
        "\r\n",
        "#df3 = df_final[list1]\r\n",
        "df3 = df_final\r\n",
        "#df3['Week_Ending'] = df_final['Week_Ending']\r\n",
        "#df3['Category'] = df_final['Category'] \r\n",
        "#df3['Target_var'] = df_final['Target_var']\r\n",
        "\r\n",
        "#dropped = list(set(variables1)-set(list1))\r\n",
        "\r\n",
        "cor = df3.corr()\r\n",
        "\r\n",
        "\r\n",
        "#Correlation with output variable\r\n",
        "cor_target = abs(cor[\"Target_var\"])\r\n",
        "\r\n",
        "#Selecting highly correlated features\r\n",
        "relevant_features = cor_target[cor_target>0.20]\r\n",
        "\r\n",
        "#Using lasso to select important features\r\n",
        "\r\n",
        "X = df3.drop([\"Target_var\",'OrderN','Category'],1)   #Feature Matrix\r\n",
        "y = df3[\"Target_var\"]\r\n",
        "\r\n",
        "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\r\n",
        "\r\n",
        "reg = LassoCV()\r\n",
        "reg.fit(X, y)\r\n",
        "#print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\r\n",
        "#print(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\r\n",
        "coef = pd.Series(reg.coef_, index = X.columns)\r\n",
        "\r\n",
        "#print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\r\n",
        "\r\n",
        "#imp_coef = coef.sort_values()\r\n",
        "#import matplotlib\r\n",
        "#matplotlib.rcParams['figure.figsize'] = (8.0, 20.0)\r\n",
        "#imp_coef.plot(kind = \"barh\")\r\n",
        "#plt.title(\"Feature importance using Lasso Model\")\r\n",
        "\r\n",
        "\r\n",
        "# ### common functions used in the below code\r\n",
        "\r\n",
        "\r\n",
        "#A helper function to find percentage change,classify anomaly based on severity.\r\n",
        "#The predict function classifies the data as anomalies based on the results from decision function on crossing a threshold.\r\n",
        "#Say if the business needs to find the next level of anomalies which might have an impact, this could be used to identify those \r\n",
        "#points.The top 30 quantiles are identified anomalies(high severity), based on decision function here we identify the 30–60 \r\n",
        "#quantile points and classify them as low severity anomalies.\r\n",
        "\r\n",
        "def classify_anomalies(df,metric_name):\r\n",
        "    df['metric_name']=metric_name\r\n",
        "    df = df.sort_values(by='OrderN', ascending=False)\r\n",
        "    #Shift actuals by one timestamp to find the percentage chage between current and previous data point\r\n",
        "    df['shift'] = df['actuals'].shift(-1)\r\n",
        "    df['percentage_change'] = ((df['actuals'] - df['shift']) / df['actuals']) * 100\r\n",
        "    return df\r\n",
        "\r\n",
        "listofcategory=df3['Category'].unique()\r\n",
        "\r\n",
        "#df3 = df3.loc[df3['Week_Ending'] < '2022-02-01']\r\n",
        "\r\n",
        "\r\n",
        "# ### Using the isolation forest algorithm for anomaly detection, explaining the anomalies using percentage change of feature values from previous week ending and then SHAP to get the important features\r\n",
        "\r\n",
        "anomaly_iforest_IQR = pd.DataFrame()\r\n",
        "featureimportance_global = pd.DataFrame()\r\n",
        "featureimportance_local = pd.DataFrame()\r\n",
        "Activity_percentage_change = pd.DataFrame()\r\n",
        "\r\n",
        "for i in listofcategory:\r\n",
        "    #print(i)\r\n",
        "    \r\n",
        "    data=df3[df3['Category']== i]\r\n",
        "    \r\n",
        "    data = data.drop(['Category'],axis=1)\r\n",
        "    data = data.groupby(['OrderN']).sum()\r\n",
        "    \r\n",
        "    data4 = data.copy()\r\n",
        "    data4['change percentage'] = data4['Target_var'].pct_change()\r\n",
        "    data4['change percentage'] = abs(data4['change percentage'])\r\n",
        "    data4['change percentage'] = data4['change percentage'].replace(np.inf, max(data4['change percentage']))\r\n",
        "    data4 = data4.fillna(0)\r\n",
        "    avgprcntgechnge = data4['change percentage'].mean()\r\n",
        "    #print(avgprcntgechnge)\r\n",
        "    n=50\r\n",
        "    data5 = data4.iloc[-n:]\r\n",
        "\r\n",
        "    count = (data5[data5['change percentage'] > avgprcntgechnge]['change percentage'].count())\r\n",
        "    contaminationfactor = count/n\r\n",
        "    #print(count)\r\n",
        "    #print(contaminationfactor)\r\n",
        "\r\n",
        "    factor = max(0.1,contaminationfactor)\r\n",
        "    #print(factor)\r\n",
        "    \r\n",
        "    model=IsolationForest(n_estimators=10, max_samples='auto', contamination=factor,                             max_features=1.0, bootstrap=False, n_jobs=-1, random_state=42, verbose=0)\r\n",
        "    model.fit(data[data.columns])\r\n",
        "    data1=data.copy()\r\n",
        "    data1['scores']=model.decision_function(data[data.columns])\r\n",
        "    data1['anomaly_iforest']=model.predict(data[data.columns])\r\n",
        "    data1.loc[data1['anomaly_iforest'] == 1,'anomaly_iforest'] = 0\r\n",
        "    data1.loc[data1['anomaly_iforest'] == -1,'anomaly_iforest'] = 1\r\n",
        "\r\n",
        "    #fig = px.line(data1,x=data1.index,y=\"Target_var\",template = 'plotly_dark')\r\n",
        "    # create list of outlier_dates\r\n",
        "    #outlier_dates = data1[data1['anomaly_iforest'] == 1].index\r\n",
        "    # obtain y value of anomalies to plot\r\n",
        "    #y_values = [data1.loc[i]['Target_var'] for i in outlier_dates]\r\n",
        "    #fig.add_trace(go.Scatter(x=outlier_dates, y=y_values, mode = 'markers', \r\n",
        "    #                name = i + \"_\" + \"Anomaly\", \r\n",
        "    #                marker=dict(color='red',size=10)))\r\n",
        "    #plt.legend(loc=\"upper center\")\r\n",
        "    #fig.show()\r\n",
        "    \r\n",
        "    high_thresh=(1.0+factor)\r\n",
        "    low_thresh=(1.0-factor)\r\n",
        "    # generate the labels using 4 week rolling average technique and IQR technique for evaluation\r\n",
        "    #data1['4W rolling average'] = abs(data1.Target_var).rolling(4).mean()\r\n",
        "    #data1['lag1'] = data1['Target_var'].shift(1)\r\n",
        "    #data1 = data1.reset_index()\r\n",
        "    #for p in range(len(data1)):\r\n",
        "    #    if data1.loc[p,'4W rolling average'] != \"nan\":\r\n",
        "    #        if (abs(data1.loc[p,'Target_var']) > (data1.loc[p,'4W rolling average']*high_thresh)) or (abs(data1.loc[p,'Target_var']) < (data1.loc[p,'4W rolling average']*low_thresh)):\r\n",
        "    #            data1.loc[p,'Anomaly_4wra'] = 1\r\n",
        "    #        else:\r\n",
        "    #            data1.loc[p,'Anomaly_4wra'] = 0      \r\n",
        "    #data1= data1.set_index('OrderN') \r\n",
        "    #data1= data1.fillna(0)\r\n",
        "\r\n",
        "    def iqr_bounds(scores):\r\n",
        "            q1 = scores.quantile((factor/2))\r\n",
        "            q3 = scores.quantile(1.0-(factor/2))\r\n",
        "            #iqr = q3 - q1\r\n",
        "            lower_bound=(q1)\r\n",
        "            upper_bound=(q3)\r\n",
        "            #print(\"Lower bound:{} \\nUpper bound:{} \\nq1:{} \\nq3:{}\".format(lower_bound,upper_bound,q1,q3))\r\n",
        "            return lower_bound,upper_bound\r\n",
        "\r\n",
        "    lower_bound,upper_bound=iqr_bounds(data1['Target_var'])\r\n",
        "\r\n",
        "    data1['Anomaly_Label_IQR']=0\r\n",
        "    data1['Anomaly_Label_IQR']=(data1['Target_var'] < lower_bound) |(data1['Target_var'] > upper_bound)\r\n",
        "    data1['Anomaly_Label_IQR']=data1['Anomaly_Label_IQR'].astype(int)  \r\n",
        "    \r\n",
        "    \r\n",
        "    # get the global feature importances at segment-product level using SHAP\r\n",
        "    exp = shap.TreeExplainer(model) #Explainer\r\n",
        "    shap_values = exp.shap_values(data)  #Calculate SHAP values\r\n",
        "    #shap.summary_plot(shap_values, data, plot_type=\"bar\", title=i+ \"-\" + \"Global importance\")\r\n",
        "    data2 = pd.DataFrame()\r\n",
        "    for j in range(0, len(data)):\r\n",
        "        data2=data2.append(dict(zip(data.columns, shap_values[j])), ignore_index=True)\r\n",
        "    \r\n",
        "    # get the local feature importances at each index level for every segment-product combination using SHAP\r\n",
        "    explainer = shap.TreeExplainer(model, data=data[data.columns])\r\n",
        "    shapvs = explainer.shap_values(data[data.columns], check_additivity=True) # check should be default\r\n",
        "    data3=pd.DataFrame()\r\n",
        "    for l in range(0,len(data)):\r\n",
        "        data3 = data3.append(dict(zip(data.columns, shapvs[l])), ignore_index=True)\r\n",
        "    \r\n",
        "    \r\n",
        "    \r\n",
        "    # visually see if the anomalies identified seem reasonable by feeding data to PCA reduced to 2 dimensions\r\n",
        "    data_2=data1.copy()\r\n",
        "    data_2=data_2.reset_index()\r\n",
        "    outliers=data_2.loc[data_2['anomaly_iforest']== 1]\r\n",
        "    outlier_index=list(outliers.index)\r\n",
        "    \r\n",
        "    #pca = PCA(2)\r\n",
        "    #pca.fit(data1[data.columns])\r\n",
        "    #res=pd.DataFrame(pca.transform(data1[data.columns]))\r\n",
        "    #Z = np.array(res)\r\n",
        "    #plt.title(\"IsolationForest\")\r\n",
        "    #plt.contourf( Z, cmap=plt.cm.Blues_r)\r\n",
        "    #b1 = plt.scatter(res[0], res[1], c='green',\r\n",
        "    #                 s=10,label=\"normal points\")\r\n",
        "    #b1 =plt.scatter(res.iloc[outlier_index,0],res.iloc[outlier_index,1], c='green',s=10,  edgecolor=\"red\")\r\n",
        "    #plt.legend(loc=\"upper center\")\r\n",
        "    #plt.show()\r\n",
        "    \r\n",
        "    # Identify anomalies for individual metrics and plot the results.\r\n",
        "    # X axis — date\r\n",
        "    # Y axis — Actual values and anomaly points.\r\n",
        "    # Actual values of metrics are indicated in the blue line and anomaly points are highlighted as red points.\r\n",
        "    # In the table, the background red indicates high anomalies and yellow indicates low anomalies.\r\n",
        "    #features_perchange=pd.DataFrame()\r\n",
        "    #for m in range(1,len(data_2.columns)-1):\r\n",
        "    #    test_df=pd.DataFrame()\r\n",
        "    #    test_df['OrderN']=data_2['OrderN']\r\n",
        "    #    test_df['actuals']=data_2.iloc[:,m:m+1]\r\n",
        "    #    test_df=classify_anomalies(test_df,data_2.columns[m])\r\n",
        "    #    features_perchange = features_perchange.append(test_df)\r\n",
        "\r\n",
        "\r\n",
        "    #outlier_dates1 = data1[data1['Anomaly_Label_IQR'] == 1].index\r\n",
        "    # obtain y value of anomalies to plot\r\n",
        "    #A_values = [data1.loc[k]['Target_var'] for k in outlier_dates1]\r\n",
        "    #fig.add_trace(go.Scatter(x=outlier_dates1, y=A_values, mode = 'markers', \r\n",
        "    #                name = i+ \"-\" + \"Anomaly IQR\",\r\n",
        "    #                marker=dict(color='yellow',size=10)))\r\n",
        "    #plt.legend(loc=\"upper center\")\r\n",
        "    #fig.update_xaxes(rangeslider_visible=True,)\r\n",
        "    #fig.show()\r\n",
        "    \r\n",
        "    #outlier_dates1 = data1[data1['Anomaly_4wra'] == 1].index\r\n",
        "    # obtain y value of anomalies to plot\r\n",
        "    #A_values = [data1.loc[k]['Target_var'] for k in outlier_dates1]\r\n",
        "    #fig.add_trace(go.Scatter(x=outlier_dates1, y=A_values, mode = 'markers', \r\n",
        "    #                name = i+ \"-\" + \"Anomaly 4wra\",\r\n",
        "    #                marker=dict(color='blue',size=10)))\r\n",
        "    #plt.legend(loc=\"upper center\")\r\n",
        "    #fig.update_xaxes(rangeslider_visible=True,)\r\n",
        "    #fig.show()\r\n",
        "    \r\n",
        "    #test_scores = emmv_scores(model, data[data.columns])\r\n",
        "    precision_IQR = precision_score(data1['Anomaly_Label_IQR'], data1['anomaly_iforest'], average='binary')\r\n",
        "    #precision_4wra = precision_score(data1['Anomaly_4wra'], data1['anomaly_iforest'], average='binary')\r\n",
        "    recall_IQR = recall_score(data1['Anomaly_Label_IQR'], data1['anomaly_iforest'], average='binary')\r\n",
        "    #recall_4wra = recall_score(data1['Anomaly_4wra'], data1['anomaly_iforest'], average='binary')\r\n",
        "    f1_IQR = f1_score(data1['Anomaly_Label_IQR'], data1['anomaly_iforest'], average='binary')\r\n",
        "    #f1_4wra = f1_score(data1['Anomaly_4wra'], data1['anomaly_iforest'], average='binary')\r\n",
        "    confusion_IQR = confusion_matrix(data1['Anomaly_Label_IQR'], data1['anomaly_iforest'])\r\n",
        "    #confusion_4wra = confusion_matrix(data1['Anomaly_4wra'], data1['anomaly_iforest'])\r\n",
        "    tn_IQR, fp_IQR, fn_IQR, tp_IQR = confusion_matrix(data1['Anomaly_Label_IQR'], data1['anomaly_iforest'], labels=[0, 1]).ravel()\r\n",
        "    #tn_4wra, fp_4wra, fn_4wra, tp_4wra = confusion_matrix(data1['Anomaly_4wra'], data1['anomaly_iforest'], labels=[0, 1]).ravel()\r\n",
        "\r\n",
        "    k=i.replace(\" \", \"_\")\r\n",
        "    k=k.replace(\"-\", \"_\")\r\n",
        "    modelname = 'Isolationforest'+k+'_model'\r\n",
        "    modelnam_type = 'Isolationforest'+k+'_model.pkl'\r\n",
        "    model_path = './Isolationforest'+k+'_model.pkl'\r\n",
        "    #print(modelname)\r\n",
        "    #print(modelnam_type)\r\n",
        "    #print(model_path)\r\n",
        "    joblib.dump(model, modelnam_type)\r\n",
        "    model = Model.register(workspace=workspace,\r\n",
        "                        model_name= modelname,                # Name of the registered model in your workspace.\r\n",
        "                        model_path= model_path,  # Local file to upload and register as a model.\r\n",
        "                        model_framework=Model.Framework.SCIKITLEARN,  # Framework used to create the model.\r\n",
        "                        model_framework_version=sklearn.__version__,  # Version of scikit-learn used to create the model.\r\n",
        "                        #sample_input_dataset=input_dataset,\r\n",
        "                        #sample_output_dataset=output_dataset,\r\n",
        "                        resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\r\n",
        "                        description='Isolation forest model for'+i,\r\n",
        "                        tags={'area': 'ASPR', 'type': 'Anomaly detection'})\r\n",
        "\r\n",
        "    #print('Name:', model.name)\r\n",
        "    #print('Version:', model.version)\r\n",
        "\r\n",
        "    #data1['em'] = test_scores['em']\r\n",
        "    #data1['mv'] = test_scores['mv']\r\n",
        "    data1['Category'] = i\r\n",
        "    data1['precision_IQR'] = precision_IQR\r\n",
        "    #data1['precision_4wra'] = precision_4wra\r\n",
        "    data1['recall_IQR'] = recall_IQR\r\n",
        "    #data1['recall_4wra'] = recall_4wra\r\n",
        "    data1['f1_IQR'] = f1_IQR\r\n",
        "    #data1['f1_4wra'] = f1_4wra\r\n",
        "    data1['tn_IQR'] = tn_IQR\r\n",
        "    data1['fp_IQR'] = fp_IQR\r\n",
        "    data1['fn_IQR'] = fn_IQR\r\n",
        "    data1['tp_IQR'] = tp_IQR\r\n",
        "    #data1['tn_4wra'] = tn_4wra\r\n",
        "    #data1['fp_4wra'] = fp_4wra\r\n",
        "    #data1['fn_4wra'] = fn_4wra\r\n",
        "    #data1['tp_4wra'] = tp_4wra\r\n",
        "    \r\n",
        "    data2['Category'] = i\r\n",
        "    data3['Category'] = i\r\n",
        "    #features_perchange['Category'] = i\r\n",
        "    data2['OrderN'] = data1.index\r\n",
        "    data2 = data2.set_index('OrderN')\r\n",
        "    data3['OrderN'] = data1.index\r\n",
        "    data3 = data3.set_index('OrderN')\r\n",
        "    #features_perchange = features_perchange.set_index('OrderN')\r\n",
        "    anomaly_iforest_IQR = anomaly_iforest_IQR.append(data1)\r\n",
        "    featureimportance_global = featureimportance_global.append(data2)\r\n",
        "    featureimportance_local = featureimportance_local.append(data3)\r\n",
        "    #Activity_percentage_change_dy = Activity_percentage_change_dy.append(features_perchange)\r\n",
        "\r\n",
        "\r\n",
        "# Extract all the files to csv\r\n",
        "local_path3 = 'Output/Isolationforest_featureimportance_local.csv'\r\n",
        "featureimportance_local.to_csv(local_path3)\r\n",
        "\r\n",
        "local_path4 = 'Output/Isolationforest_featureimportance_global.csv'\r\n",
        "featureimportance_global.to_csv(local_path4)\r\n",
        "\r\n",
        "local_path5 = 'Output/Isolationforest_results.csv'\r\n",
        "anomaly_iforest_IQR.to_csv(local_path5)\r\n",
        "\r\n",
        "\r\n",
        "# ### Using the 1 class SVM algorithm for anomaly detection, explaining the anomalies using percentage change of feature values from previous week ending\r\n",
        "\r\n",
        "anomaly_1clssvm_IQR = pd.DataFrame()\r\n",
        "featureimportance_global_1clssvm = pd.DataFrame()\r\n",
        "featureimportance_local_1clssvm = pd.DataFrame()\r\n",
        "#Activity_percentage_change_1clssvm = pd.DataFrame()\r\n",
        "\r\n",
        "for i in listofcategory:\r\n",
        "    #print(i)\r\n",
        "    \r\n",
        "    data=df3[df3['Category']== i]\r\n",
        "    \r\n",
        "    data = data.drop(['Category'],axis=1)\r\n",
        "    data = data.groupby(['OrderN']).sum()\r\n",
        "    \r\n",
        "    model = OneClassSVM(kernel = 'rbf', gamma = 0.001, nu = 0.01)\r\n",
        "    model.fit(data[data.columns])\r\n",
        "    data1=data.copy()\r\n",
        "    data1['scores']=model.decision_function(data[data.columns])\r\n",
        "    data1['anomaly_1clssvm']=model.predict(data[data.columns])\r\n",
        "    data1.loc[data1['anomaly_1clssvm'] == 1,'anomaly_1clssvm'] = 0\r\n",
        "    data1.loc[data1['anomaly_1clssvm'] == -1,'anomaly_1clssvm'] = 1\r\n",
        "\r\n",
        "    #fig = px.line(data1, x=data1.index, y=\"Target_var\",  template = 'plotly_dark')\r\n",
        "\r\n",
        "    # create list of outlier_dates\r\n",
        "    #outlier_dates = data1[data1['anomaly_1clssvm'] == 1].index\r\n",
        "    # obtain y value of anomalies to plot\r\n",
        "    #y_values = [data1.loc[i]['Target_var'] for i in outlier_dates]\r\n",
        "    #fig.add_trace(go.Scatter(x=outlier_dates, y=y_values, mode = 'markers', \r\n",
        "    #                name = i + \"_\" + \"Anomaly\", \r\n",
        "    #                marker=dict(color='red',size=10)))\r\n",
        "\r\n",
        "    #fig.show()\r\n",
        "    \r\n",
        "    data7=data1.copy()\r\n",
        "    data7['change percentage'] = data7['Target_var'].pct_change()\r\n",
        "    data7['change percentage'] = abs(data7['change percentage'])\r\n",
        "    data7['change percentage'] = data7['change percentage'].replace(np.inf, max(data7['change percentage']))\r\n",
        "    data7 = data7.fillna(0)\r\n",
        "    avgprcntgechnge1 = data7['change percentage'].mean()\r\n",
        "    #print(avgprcntgechnge1)\r\n",
        "    n1=50\r\n",
        "    data8 = data7.iloc[-n1:]\r\n",
        "    count1 = (data8[data8['change percentage'] > avgprcntgechnge1]['change percentage'].count())\r\n",
        "    contaminationfactor1 = count1/n1\r\n",
        "    #print(count1)\r\n",
        "    #print(contaminationfactor1)\r\n",
        "    factor1 = max(0.1,contaminationfactor1)\r\n",
        "    #print(factor1)\r\n",
        "    high_thresh1=(1.0+factor1)\r\n",
        "    low_thresh1=(1.0-factor1)\r\n",
        "    \r\n",
        "    # generate the labels using 4 week rolling average technique and IQR technique for evaluation\r\n",
        "    #data1['4W rolling average'] = abs(data1.Target_var).rolling(4).mean()\r\n",
        "    #data1['lag1'] = data1['Target_var'].shift(1)\r\n",
        "    #data1 = data1.reset_index()\r\n",
        "    #for p in range(len(data1)):\r\n",
        "    #    if data1.loc[p,'4W rolling average'] != \"nan\":\r\n",
        "    #        if (abs(data1.loc[p,'Target_var']) > (data1.loc[p,'4W rolling average']*high_thresh1)) or (abs(data1.loc[p,'Target_var']) < (data1.loc[p,'4W rolling average']*low_thresh1)):\r\n",
        "    #            data1.loc[p,'Anomaly_4wra'] = 1\r\n",
        "    #        else:\r\n",
        "    #            data1.loc[p,'Anomaly_4wra'] = 0      \r\n",
        "    #data1= data1.set_index('Week_Ending') \r\n",
        "    #data1= data1.fillna(0)\r\n",
        "\r\n",
        "    def iqr_bounds(scores):\r\n",
        "            q1 = scores.quantile((factor1/2))\r\n",
        "            q3 = scores.quantile(1-(factor1/2))\r\n",
        "            #iqr = q3 - q1\r\n",
        "            lower_bound=(q1)\r\n",
        "            upper_bound=(q3)\r\n",
        "            #print(\"Lower bound:{} \\nUpper bound:{} \\nq1:{} \\nq3:{}\".format(lower_bound,upper_bound,q1,q3))\r\n",
        "            return lower_bound,upper_bound\r\n",
        "\r\n",
        "    lower_bound,upper_bound=iqr_bounds(data1['Target_var'])\r\n",
        "\r\n",
        "    data1['Anomaly_Label_IQR']=0\r\n",
        "    data1['Anomaly_Label_IQR']=(data1['Target_var'] < lower_bound) |(data1['Target_var'] > upper_bound)\r\n",
        "    data1['Anomaly_Label_IQR']=data1['Anomaly_Label_IQR'].astype(int) \r\n",
        "    \r\n",
        "    # SVM is not supported by tree explainer hence we cant get feature importance using SHAP here.\r\n",
        "    \r\n",
        "    \r\n",
        "    # visually see if the anomalies identified seem reasonable using by feeding data to PCA reduced to 2 dimensions\r\n",
        "    data_2=data1.copy()\r\n",
        "    data_2=data_2.reset_index()\r\n",
        "    outliers=data_2.loc[data_2['anomaly_1clssvm']== 1]\r\n",
        "    outlier_index=list(outliers.index)\r\n",
        "    \r\n",
        "    #pca = PCA(2)\r\n",
        "    #pca.fit(data1[data.columns])\r\n",
        "    #res=pd.DataFrame(pca.transform(data1[data.columns]))\r\n",
        "    #Z = np.array(res)\r\n",
        "    #plt.title(\"One Class SVM\")\r\n",
        "    #plt.contourf( Z, cmap=plt.cm.Blues_r)\r\n",
        "    #b1 = plt.scatter(res[0], res[1], c='green',\r\n",
        "    #                 s=20,label=\"normal points\")\r\n",
        "    #b1 =plt.scatter(res.iloc[outlier_index,0],res.iloc[outlier_index,1], c='green',s=20,  edgecolor=\"red\",label=\"predicted outliers\")\r\n",
        "    #plt.legend(loc=\"upper right\")\r\n",
        "    #plt.show()\r\n",
        "    \r\n",
        "    # Identify anomalies for individual metrics and plot the results.\r\n",
        "    # X axis — date\r\n",
        "    # Y axis — Actual values and anomaly points.\r\n",
        "    # Actual values of metrics are indicated in the blue line and anomaly points are highlighted as red points.\r\n",
        "    # In the table, the background red indicates high anomalies and yellow indicates low anomalies.\r\n",
        "    \r\n",
        "    #features_perchange=pd.DataFrame()\r\n",
        "    #for m in range(1,len(data_2.columns)-1):\r\n",
        "    #    model.fit(data_2.iloc[:,m:m+1])\r\n",
        "    #    pred = model.predict(data_2.iloc[:,m:m+1])\r\n",
        "    #    test_df=pd.DataFrame()\r\n",
        "    #    test_df['Week_Ending']=data_2['Week_Ending']\r\n",
        "    #    #Find decision function to find the score and classify anomalies\r\n",
        "    #    test_df['score']=model.decision_function(data_2.iloc[:,m:m+1])\r\n",
        "    #    test_df['actuals']=data_2.iloc[:,m:m+1]\r\n",
        "    #    test_df['anomaly']=pred\r\n",
        "    #    #Get the indexes of outliers in order to compare the metrics with use case anomalies if required\r\n",
        "    #    outliers=test_df.loc[test_df['anomaly']==-1]\r\n",
        "    #    outlier_index=list(outliers.index)\r\n",
        "    #    test_df=classify_anomalies(test_df,data_2.columns[m])\r\n",
        "    #    features_perchange = features_perchange.append(test_df)\r\n",
        "\r\n",
        "\r\n",
        "    #outlier_dates1 = data1[data1['Anomaly_Label_IQR'] == 1].index\r\n",
        "    # obtain y value of anomalies to plot\r\n",
        "    #A_values = [data1.loc[k]['Target_var'] for k in outlier_dates1]\r\n",
        "    #fig.add_trace(go.Scatter(x=outlier_dates1, y=A_values, mode = 'markers', \r\n",
        "    #                name = i+ \"-\" + \"Anomaly IQR\", \r\n",
        "    #                marker=dict(color='yellow',size=20)))\r\n",
        "    #fig.update_xaxes(rangeslider_visible=True,)\r\n",
        "    #fig.show()\r\n",
        "    \r\n",
        "    precision_IQR = precision_score(data1['Anomaly_Label_IQR'], data1['anomaly_1clssvm'], average='binary')\r\n",
        "    #precision_4wra = precision_score(data1['Anomaly_4wra'], data1['anomaly_1clssvm'], average='binary')\r\n",
        "    recall_IQR = recall_score(data1['Anomaly_Label_IQR'], data1['anomaly_1clssvm'], average='binary')\r\n",
        "    #recall_4wra = recall_score(data1['Anomaly_4wra'], data1['anomaly_1clssvm'], average='binary')\r\n",
        "    f1_IQR = f1_score(data1['Anomaly_Label_IQR'], data1['anomaly_1clssvm'], average='binary')\r\n",
        "    #f1_4wra = f1_score(data1['Anomaly_4wra'], data1['anomaly_1clssvm'], average='binary')\r\n",
        "    tn_IQR, fp_IQR, fn_IQR, tp_IQR = confusion_matrix(data1['Anomaly_Label_IQR'], data1['anomaly_1clssvm'], labels=[0, 1]).ravel()\r\n",
        "    #tn_4wra, fp_4wra, fn_4wra, tp_4wra = confusion_matrix(data1['Anomaly_4wra'], data1['anomaly_1clssvm'], labels=[0, 1]).ravel()\r\n",
        "\r\n",
        "    k=i.replace(\" \", \"_\")\r\n",
        "    k=k.replace(\"-\", \"_\")\r\n",
        "    modelname = '1classvm'+k+'_model'\r\n",
        "    modelnam_type = '1classvm'+k+'_model.pkl'\r\n",
        "    model_path = './1classvm'+k+'_model.pkl'\r\n",
        "    #print(modelname)\r\n",
        "    #print(modelnam_type)\r\n",
        "    #print(model_path)\r\n",
        "    joblib.dump(model, modelnam_type)\r\n",
        "    model = Model.register(workspace=workspace,\r\n",
        "                        model_name= modelname,                # Name of the registered model in your workspace.\r\n",
        "                        model_path= model_path,  # Local file to upload and register as a model.\r\n",
        "                        model_framework=Model.Framework.SCIKITLEARN,  # Framework used to create the model.\r\n",
        "                        model_framework_version=sklearn.__version__,  # Version of scikit-learn used to create the model.\r\n",
        "                        #sample_input_dataset=input_dataset,\r\n",
        "                        #sample_output_dataset=output_dataset,\r\n",
        "                        resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\r\n",
        "                        description='One class SVM model for'+i,\r\n",
        "                        tags={'area': 'ASPR', 'type': 'Anomaly detection'})\r\n",
        "\r\n",
        "    #print('Name:', model.name)\r\n",
        "    #print('Version:', model.version)\r\n",
        "\r\n",
        "    data1['Category'] = i\r\n",
        "    data1['precision_IQR'] = precision_IQR\r\n",
        "    #data1['precision_4wra'] = precision_4wra\r\n",
        "    data1['recall_IQR'] = recall_IQR\r\n",
        "    #data1['recall_4wra'] = recall_4wra\r\n",
        "    data1['f1_IQR'] = f1_IQR\r\n",
        "    #data1['f1_4wra'] = f1_4wra\r\n",
        "    data1['tn_IQR'] = tn_IQR\r\n",
        "    data1['fp_IQR'] = fp_IQR\r\n",
        "    data1['fn_IQR'] = fn_IQR\r\n",
        "    data1['tp_IQR'] = tp_IQR\r\n",
        "    #data1['tn_4wra'] = tn_4wra\r\n",
        "    #data1['fp_4wra'] = fp_4wra\r\n",
        "    #data1['fn_4wra'] = fn_4wra\r\n",
        "    #data1['tp_4wra'] = tp_4wra\r\n",
        "    #data1['Category'] = i\r\n",
        "    #features_perchange['Category'] = i\r\n",
        "    #features_perchange = features_perchange.set_index('Week_Ending')\r\n",
        "    anomaly_1clssvm_IQR = anomaly_1clssvm_IQR.append(data1)\r\n",
        "    #Activity_percentage_change_1clssvm = Activity_percentage_change_1clssvm.append(features_perchange)\r\n",
        "\r\n",
        "# Extract all the files to csv\r\n",
        "\r\n",
        "local_path6 = 'Output/1clssvm_results.csv'\r\n",
        "anomaly_1clssvm_IQR.to_csv(local_path6)\r\n",
        "\r\n",
        "#local_path7 = 'Output/Activity_percentage_change_1clssvm.csv'\r\n",
        "#Activity_percentage_change_1clssvm.to_csv(local_path7)\r\n",
        "\r\n",
        "\r\n",
        "anomaly_1clssvm_IQR.drop(['Anomaly_Label_IQR','f1_IQR','precision_IQR','recall_IQR','tn_IQR','fp_IQR','fn_IQR','tp_IQR'], inplace=True, axis=1)\r\n",
        "#anomaly_1clssvm_IQR = anomaly_1clssvm_IQR.rename(columns = {'SIO_Growth_x':'SIO_Growth'}, inplace = False)\r\n",
        "\r\n",
        "# #### RUN random forest classifier using arima labels\r\n",
        "\r\n",
        "anomaly_RF_IQR = pd.DataFrame()\r\n",
        "oneclssvm_featureimportance_global = pd.DataFrame()\r\n",
        "oneclssvm_featureimportance_local = pd.DataFrame()\r\n",
        "#1clssvm_Activity_percentage_change = pd.DataFrame()\r\n",
        "\r\n",
        "for i in listofcategory:\r\n",
        "    \r\n",
        "    data=anomaly_1clssvm_IQR[anomaly_1clssvm_IQR['Category']== i]\r\n",
        "    \r\n",
        "    data = data.drop(['Category'],axis=1)\r\n",
        "    data = data.groupby(['OrderN']).sum()\r\n",
        "    \r\n",
        "    data4 = data.copy()\r\n",
        "    data4['change percentage'] = data4['Target_var'].pct_change()\r\n",
        "    data4['change percentage'] = abs(data4['change percentage'])\r\n",
        "    data4['change percentage'] = data4['change percentage'].replace(np.inf, max(data4['change percentage']))\r\n",
        "    data4 = data4.fillna(0)\r\n",
        "    avgprcntgechnge = data4['change percentage'].mean()\r\n",
        "    n=50\r\n",
        "    data6 = data4.iloc[-n:]\r\n",
        "\r\n",
        "    count = (data6[data6['change percentage'] > avgprcntgechnge]['change percentage'].count())\r\n",
        "    contaminationfactor = count/n\r\n",
        "\r\n",
        "    factor = max(0.1,contaminationfactor)\r\n",
        "    \r\n",
        "    high_thresh=(1.0+factor)\r\n",
        "    low_thresh=(1.0-factor)\r\n",
        "\r\n",
        "    # split into X and y\r\n",
        "    X = data4.drop(['anomaly_1clssvm','change percentage'], axis=1)\r\n",
        "    y = data4['anomaly_1clssvm']\r\n",
        "    X = X.reset_index()\r\n",
        "    y = y.reset_index()\r\n",
        "    X = X.groupby(['OrderN']).sum()\r\n",
        "    y = y.groupby(['OrderN']).sum()\r\n",
        "    \r\n",
        "    data7=X.copy()\r\n",
        "    data7['change percentage'] = data7['Target_var'].pct_change()\r\n",
        "    data7['change percentage'] = abs(data7['change percentage'])\r\n",
        "    data7 = data7.fillna(0)\r\n",
        "    avgprcntgechnge1 = data7['change percentage'].mean()\r\n",
        "    n1=50\r\n",
        "    data8 = data7.iloc[-n1:]\r\n",
        "    count1 = (data8[data8['change percentage'] > avgprcntgechnge1]['change percentage'].count())\r\n",
        "    contaminationfactor1 = count1/n1\r\n",
        "    factor1 = max(0.1,contaminationfactor1)\r\n",
        "    high_thresh1=(1.0+factor1)\r\n",
        "    low_thresh1=(1.0-factor1)\r\n",
        "    \r\n",
        "    X = X.drop(['Target_var'], axis=1)\r\n",
        "    RF_model = RandomForestRegressor()\r\n",
        "    RF = RF_model.fit(X,y)\r\n",
        "    data1=X.copy()\r\n",
        "    data1['anomaly_RF'] = RF.predict(X)\r\n",
        "    data1['anomaly_1classvm']=y['anomaly_1clssvm']\r\n",
        "\r\n",
        "    # get the global feature importances at segment-product level using SHAP\r\n",
        "    exp = shap.TreeExplainer(RF) #Explainer\r\n",
        "    data = data.drop(['Target_var'], axis=1)\r\n",
        "    shap_values = exp.shap_values(data)  #Calculate SHAP values\r\n",
        "    #shap.summary_plot(shap_values, data, plot_type=\"bar\", title=i+ \"-\" + \"Global importance\")\r\n",
        "    data2 = pd.DataFrame()\r\n",
        "    for j in range(0, len(data)):\r\n",
        "        data2=data2.append(dict(zip(data.columns, shap_values[j])), ignore_index=True)\r\n",
        "\r\n",
        "    # get the local feature importances at each index level for every segment-product combination using SHAP\r\n",
        "    #data = data.drop(['Target_var'], axis=1)\r\n",
        "    explainer = shap.TreeExplainer(RF, data=data[data.columns])\r\n",
        "    shapvs = explainer.shap_values(data[data.columns], check_additivity=True) # check should be default\r\n",
        "\r\n",
        "    data3=pd.DataFrame()\r\n",
        "    for l in range(0,len(data)):\r\n",
        "        data3 = data3.append(dict(zip(data.columns, shapvs[l])), ignore_index=True) \r\n",
        "        \r\n",
        "    data_2=data1.copy()\r\n",
        "    data_2['Target_var'] = data7['Target_var']\r\n",
        "    data_2=data_2.reset_index()\r\n",
        "    #features_perchange=pd.DataFrame()\r\n",
        "    #for m in range(1,len(data_2.columns)-1):\r\n",
        "    #    test_df=pd.DataFrame()\r\n",
        "    #    test_df['Week_Ending']=data_2['Week_Ending']\r\n",
        "    #    test_df['actuals']=data_2.iloc[:,m:m+1]\r\n",
        "    #    test_df=classify_anomalies(test_df,data_2.columns[m])\r\n",
        "    #    features_perchange = features_perchange.append(test_df)\r\n",
        "    \r\n",
        "    for n in data1.index:\r\n",
        "        if (data1.loc[n,'anomaly_RF'] > 0.1):\r\n",
        "            data1.loc[n,'anomaly_RF'] = 1\r\n",
        "        else:\r\n",
        "            data1.loc[n,'anomaly_RF'] = 0\r\n",
        "    \r\n",
        "    precision_IQR = precision_score(data1['anomaly_1classvm'], data1['anomaly_RF'])\r\n",
        "    recall_IQR = recall_score(data1['anomaly_1classvm'], data1['anomaly_RF'])\r\n",
        "    f1_IQR = f1_score(data1['anomaly_1classvm'], data1['anomaly_RF'])\r\n",
        "    confusion_IQR = confusion_matrix(data1['anomaly_1classvm'], data1['anomaly_RF'])\r\n",
        "    tn_IQR, fp_IQR, fn_IQR, tp_IQR = confusion_matrix(data1['anomaly_1classvm'], data1['anomaly_RF'], labels=[0, 1]).ravel()\r\n",
        "    \r\n",
        "    data1['Target_var'] = data7['Target_var']    \r\n",
        "    \r\n",
        "    data1['Category'] = i\r\n",
        "    data1['precision_IQR'] = precision_IQR\r\n",
        "    data1['recall_IQR'] = recall_IQR\r\n",
        "    data1['f1_IQR'] = f1_IQR\r\n",
        "    data1['tn_IQR'] = tn_IQR\r\n",
        "    data1['fp_IQR'] = fp_IQR\r\n",
        "    data1['fn_IQR'] = fn_IQR\r\n",
        "    data1['tp_IQR'] = tp_IQR\r\n",
        "    data3['Category'] = i\r\n",
        "    data3['OrderN'] = data1.index\r\n",
        "    data3 = data3.set_index('OrderN')\r\n",
        "    data2['Category'] = i\r\n",
        "    data2['OrderN'] = data1.index\r\n",
        "    data2 = data2.set_index('OrderN')\r\n",
        "    oneclssvm_featureimportance_global = oneclssvm_featureimportance_global.append(data2)\r\n",
        "    anomaly_RF_IQR = anomaly_RF_IQR.append(data1)\r\n",
        "    oneclssvm_featureimportance_local = oneclssvm_featureimportance_local.append(data3)\r\n",
        "    #1clssvm_Activity_percentage_change = 1clssvm_Activity_percentage_change.append(features_perchange)\r\n",
        "\r\n",
        "\r\n",
        "# Extract all the files to csv\r\n",
        "\r\n",
        "local_path7 = 'Output/oneclssvm_featureimportance_local.csv'\r\n",
        "oneclssvm_featureimportance_local.to_csv(local_path7)\r\n",
        "\r\n",
        "local_path8 = 'Output/oneclssvm_featureimportance_global.csv'\r\n",
        "oneclssvm_featureimportance_global.to_csv(local_path8)\r\n",
        "\r\n",
        "#local_path3 = 'Output/Arima_Activity_percentage_change.csv'\r\n",
        "#Arima_Activity_percentage_change.to_csv(local_path3)\r\n",
        "\r\n",
        "\r\n",
        "all_results = pd.DataFrame()\r\n",
        "all_results = anomaly_iforest_IQR[['Category']]\r\n",
        "all_results['IForest_f1_IQR'] = anomaly_iforest_IQR['f1_IQR']\r\n",
        "all_results['SVM_f1_IQR'] = anomaly_1clssvm_IQR['f1_IQR']\r\n",
        "all_results = all_results.reset_index()\r\n",
        "#all_results['RNN_f1_IQR'] = anomaly_RNN_IQR['f1_IQR']\r\n",
        "#all_results['Arima_f1_IQR'] = arima_results['f1_IQR']\r\n",
        "\r\n",
        "all_results = all_results.groupby(['Category']).mean()\r\n",
        "\r\n",
        "#var_results = var_results.groupby(['Segment-Product']).mean()\r\n",
        "#all_results['Var_f1_IQR']=var_results['f1_IQR']\r\n",
        "\r\n",
        "all_results['Best_model'] = all_results.idxmax(axis=1)\r\n",
        "\r\n",
        "best_model_across_segments = all_results['Best_model'].mode()\r\n",
        "\r\n",
        "local_path9 = 'Output/all_results.csv'\r\n",
        "all_results.to_csv(local_path9)\r\n",
        "\r\n",
        "# get the datastore to upload prepared data\r\n",
        "datastore = workspace.get_default_datastore()\r\n",
        "\r\n",
        "# upload the local file from src_dir to the target_path in datastore\r\n",
        "datastore.upload(src_dir = 'Output', target_path = 'ASPR_Anomaly_Detection' ,overwrite=True)\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ASPR_pipeline/aspr_script.py\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\r\n",
        "from azureml.core.runconfig import RunConfiguration\r\n",
        "from azureml.core.runconfig import DockerConfiguration\r\n",
        "#from azureml.core import ScriptRunConfig\r\n",
        "\r\n",
        "#Create a Python environment for the experiment (from a .yml file)\r\n",
        "\r\n",
        "experiment_env = Environment.from_conda_specification(\"python_env2\", experiment_folder + \"/python_env2.yml\")\r\n",
        "\r\n",
        "\r\n",
        "experiment_env.register(workspace=ws)\r\n",
        "\r\n",
        "# registered_env = Environment.get(ws, 'python_env2')\r\n",
        "registered_env = Environment.get(ws,name=\"python_env2\",version=\"18\")\r\n",
        "\r\n",
        "# Create a new runconfig object for the pipeline\r\n",
        "pipeline_run_config = RunConfiguration()\r\n",
        "\r\n",
        "# Use the compute you created above. \r\n",
        "pipeline_run_config.target = pipeline_cluster\r\n",
        "\r\n",
        "# Assign the environment to the run configuration\r\n",
        "pipeline_run_config.environment = registered_env\r\n",
        "\r\n",
        "print (\"Run configuration created.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "No Python version provided, defaulting to \"3.8.12\"\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1659951703717
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.data import OutputFileDatasetConfig\r\n",
        "from azureml.pipeline.steps import PythonScriptStep\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# Run the  train script\r\n",
        "train_step = PythonScriptStep(name = \"train model\",\r\n",
        "                                source_directory = experiment_folder ,\r\n",
        "                                script_name = \"aspr_script.py\",\r\n",
        "                                #compute_target = pipeline_cluster,\r\n",
        "                                runconfig = pipeline_run_config,\r\n",
        "                                allow_reuse = False)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1659951704618
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\r\n",
        "from azureml.pipeline.core import Pipeline\r\n",
        "from azureml.widgets import RunDetails\r\n",
        "\r\n",
        "\r\n",
        "# Construct the pipeline\r\n",
        "pipeline_steps = [train_step]\r\n",
        "pipeline = Pipeline(workspace=ws, steps=pipeline_steps,description=\"Traning\")\r\n",
        "print(\"Pipeline is built.\")\r\n",
        "\r\n",
        "\r\n",
        "# Create an experiment and run the pipeline\r\n",
        "experiment = Experiment(workspace=ws, name = 'ASPR_experiment')\r\n",
        "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\r\n",
        "print(\"Pipeline submitted for execution.\")\r\n",
        "#RunDetails(pipeline_run).show()\r\n",
        "\r\n",
        "pipeline_run.wait_for_completion(show_output=True)\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline is built.\nCreated step train model [f1e42c26][58bb0a63-aead-4c64-bb8c-69194feb6238], (This step will run and generate new outputs)\nSubmitted PipelineRun 023980d1-4262-40d4-9a72-ab4572c51559\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/023980d1-4262-40d4-9a72-ab4572c51559?wsid=/subscriptions/c4f0bb45-07a9-489a-8f6a-6fbc64f25b46/resourcegroups/tcp-azu0044-ae-rg-dcml-uc001-dt/workspaces/tcp0044aedcmluc001dtmlws&tid=49dfc6a3-5fb7-49f4-adea-c54e725bb854\nPipeline submitted for execution.\nPipelineRunId: 023980d1-4262-40d4-9a72-ab4572c51559\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/023980d1-4262-40d4-9a72-ab4572c51559?wsid=/subscriptions/c4f0bb45-07a9-489a-8f6a-6fbc64f25b46/resourcegroups/tcp-azu0044-ae-rg-dcml-uc001-dt/workspaces/tcp0044aedcmluc001dtmlws&tid=49dfc6a3-5fb7-49f4-adea-c54e725bb854\nPipelineRun Status: NotStarted\nPipelineRun Status: Running\n\n\nStepRunId: 98fbc6bd-6c99-4413-81e3-20a86901658d\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/98fbc6bd-6c99-4413-81e3-20a86901658d?wsid=/subscriptions/c4f0bb45-07a9-489a-8f6a-6fbc64f25b46/resourcegroups/tcp-azu0044-ae-rg-dcml-uc001-dt/workspaces/tcp0044aedcmluc001dtmlws&tid=49dfc6a3-5fb7-49f4-adea-c54e725bb854\nStepRun( train model ) Status: Running\n\nStepRun(train model) Execution Summary\n=======================================\nStepRun( train model ) Status: Failed\n\nWarnings:\n{\n  \"error\": {\n    \"code\": \"UserError\",\n    \"severity\": null,\n    \"message\": \"AzureMLCompute job failed.\\nExecutionFailed: [REDACTED]\\n\\texit_codes: 1\",\n    \"messageFormat\": \"{Message}\",\n    \"messageParameters\": {\n      \"Message\": \"AzureMLCompute job failed.\\nExecutionFailed: [REDACTED]\\n\\texit_codes: 1\"\n    },\n    \"referenceCode\": null,\n    \"detailsUri\": null,\n    \"target\": null,\n    \"details\": [],\n    \"innerError\": {\n      \"code\": \"UserTrainingScriptFailed\",\n      \"innerError\": null\n    },\n    \"debugInfo\": null,\n    \"additionalInfo\": null\n  },\n  \"correlation\": {\n    \"operation\": \"3f13cc57e84931911ea25b54f49c180b\",\n    \"request\": \"c5df0e6aaaf07fe2\"\n  },\n  \"environment\": \"australiaeast\",\n  \"location\": \"australiaeast\",\n  \"time\": \"2022-08-08T12:24:33.6012197+00:00\",\n  \"componentName\": \"globaljobdispatcher\"\n}\n{\n  \"error\": {\n    \"code\": \"UserError\",\n    \"severity\": null,\n    \"message\": \"{\\\"NonCompliant\\\":\\\"Process '/azureml-envs/azureml_2985c9e26d721291a108505866e36889/bin/python' exited with code 1 and error message 'Execution failed. Process exited with status code 1. Error: Traceback (most recent call last):\\\\n  File \\\\\\\"aspr_script.py\\\\\\\", l\",\n    \"messageFormat\": null,\n    \"messageParameters\": {},\n    \"referenceCode\": null,\n    \"detailsUri\": null,\n    \"target\": null,\n    \"details\": [],\n    \"innerError\": null,\n    \"debugInfo\": null,\n    \"additionalInfo\": null\n  },\n  \"correlation\": null,\n  \"environment\": null,\n  \"location\": null,\n  \"time\": \"0001-01-01T00:00:00+00:00\",\n  \"componentName\": null\n}\n"
        },
        {
          "output_type": "error",
          "ename": "ActivityFailedException",
          "evalue": "ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"{\\\"NonCompliant\\\":\\\"Process '/azureml-envs/azureml_2985c9e26d721291a108505866e36889/bin/python' exited with code 1 and error message 'Execution failed. Process exited with status code 1. Error: Traceback (most recent call last):\\\\n  File \\\\\\\"aspr_script.py\\\\\\\", line 875, in <module>\\\\n    all_results['SVM_f1_IQR'] = anomaly_1clssvm_IQR['f1_IQR']\\\\n  File \\\\\\\"/azureml-envs/azureml_2985c9e26d721291a108505866e36889/lib/python3.6/site-packages/pandas/core/frame.py\\\\\\\", line 2906, in __getitem__\\\\n    indexer = self.columns.get_loc(key)\\\\n  File \\\\\\\"/azureml-envs/azureml_2985c9e26d721291a108505866e36889/lib/python3.6/site-packages/pandas/core/indexes/base.py\\\\\\\", line 2900, in get_loc\\\\n    raise KeyError(key) from err\\\\nKeyError: 'f1_IQR'\\\\n\\\\n'. Please check the log file 'user_logs/std_log.txt' for more details.\\\"}\\n{\\n  \\\"code\\\": \\\"ExecutionFailed\\\",\\n  \\\"target\\\": \\\"\\\",\\n  \\\"category\\\": \\\"UserError\\\",\\n  \\\"error_details\\\": [\\n    {\\n      \\\"key\\\": \\\"exit_codes\\\",\\n      \\\"value\\\": \\\"1\\\"\\n    }\\n  ]\\n}\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"{\\\\\\\"NonCompliant\\\\\\\":\\\\\\\"Process '/azureml-envs/azureml_2985c9e26d721291a108505866e36889/bin/python' exited with code 1 and error message 'Execution failed. Process exited with status code 1. Error: Traceback (most recent call last):\\\\\\\\n  File \\\\\\\\\\\\\\\"aspr_script.py\\\\\\\\\\\\\\\", line 875, in <module>\\\\\\\\n    all_results['SVM_f1_IQR'] = anomaly_1clssvm_IQR['f1_IQR']\\\\\\\\n  File \\\\\\\\\\\\\\\"/azureml-envs/azureml_2985c9e26d721291a108505866e36889/lib/python3.6/site-packages/pandas/core/frame.py\\\\\\\\\\\\\\\", line 2906, in __getitem__\\\\\\\\n    indexer = self.columns.get_loc(key)\\\\\\\\n  File \\\\\\\\\\\\\\\"/azureml-envs/azureml_2985c9e26d721291a108505866e36889/lib/python3.6/site-packages/pandas/core/indexes/base.py\\\\\\\\\\\\\\\", line 2900, in get_loc\\\\\\\\n    raise KeyError(key) from err\\\\\\\\nKeyError: 'f1_IQR'\\\\\\\\n\\\\\\\\n'. Please check the log file 'user_logs/std_log.txt' for more details.\\\\\\\"}\\\\n{\\\\n  \\\\\\\"code\\\\\\\": \\\\\\\"ExecutionFailed\\\\\\\",\\\\n  \\\\\\\"target\\\\\\\": \\\\\\\"\\\\\\\",\\\\n  \\\\\\\"category\\\\\\\": \\\\\\\"UserError\\\\\\\",\\\\n  \\\\\\\"error_details\\\\\\\": [\\\\n    {\\\\n      \\\\\\\"key\\\\\\\": \\\\\\\"exit_codes\\\\\\\",\\\\n      \\\\\\\"value\\\\\\\": \\\\\\\"1\\\\\\\"\\\\n    }\\\\n  ]\\\\n}\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
            "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline submitted for execution.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#RunDetails(pipeline_run).show()\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mpipeline_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshow_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/run.py:295\u001b[0m, in \u001b[0;36mPipelineRun.wait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m     \u001b[43mstep_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime_elapsed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mraise_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_on_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# If there are package conflicts in the user's environment, the run rehydration\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# will not work and we will receive a Run object instead of StepRun.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;66;03m# Run.wait_for_completion() does not have a parameter timeout_seconds, which\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# will generate a TypeError here.  As a workaround, call the method without\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m# this parameter.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(step_run, StepRun):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/run.py:738\u001b[0m, in \u001b[0;36mStepRun.wait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show_output:\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 738\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_run_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mraise_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_on_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    741\u001b[0m         error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe output streaming for the run interrupted.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    742\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut the run is still executing on the compute target. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    743\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetails for canceling the run can be found here: \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    744\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://aka.ms/aml-docs-cancel-run\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/run.py:831\u001b[0m, in \u001b[0;36mStepRun._stream_run_output\u001b[0;34m(self, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(error, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mand\u001b[39;00m raise_on_error:\n\u001b[0;32m--> 831\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ActivityFailedException(error_details\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(error, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_details)\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[0;31mActivityFailedException\u001b[0m: ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"{\\\"NonCompliant\\\":\\\"Process '/azureml-envs/azureml_2985c9e26d721291a108505866e36889/bin/python' exited with code 1 and error message 'Execution failed. Process exited with status code 1. Error: Traceback (most recent call last):\\\\n  File \\\\\\\"aspr_script.py\\\\\\\", line 875, in <module>\\\\n    all_results['SVM_f1_IQR'] = anomaly_1clssvm_IQR['f1_IQR']\\\\n  File \\\\\\\"/azureml-envs/azureml_2985c9e26d721291a108505866e36889/lib/python3.6/site-packages/pandas/core/frame.py\\\\\\\", line 2906, in __getitem__\\\\n    indexer = self.columns.get_loc(key)\\\\n  File \\\\\\\"/azureml-envs/azureml_2985c9e26d721291a108505866e36889/lib/python3.6/site-packages/pandas/core/indexes/base.py\\\\\\\", line 2900, in get_loc\\\\n    raise KeyError(key) from err\\\\nKeyError: 'f1_IQR'\\\\n\\\\n'. Please check the log file 'user_logs/std_log.txt' for more details.\\\"}\\n{\\n  \\\"code\\\": \\\"ExecutionFailed\\\",\\n  \\\"target\\\": \\\"\\\",\\n  \\\"category\\\": \\\"UserError\\\",\\n  \\\"error_details\\\": [\\n    {\\n      \\\"key\\\": \\\"exit_codes\\\",\\n      \\\"value\\\": \\\"1\\\"\\n    }\\n  ]\\n}\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"{\\\\\\\"NonCompliant\\\\\\\":\\\\\\\"Process '/azureml-envs/azureml_2985c9e26d721291a108505866e36889/bin/python' exited with code 1 and error message 'Execution failed. Process exited with status code 1. Error: Traceback (most recent call last):\\\\\\\\n  File \\\\\\\\\\\\\\\"aspr_script.py\\\\\\\\\\\\\\\", line 875, in <module>\\\\\\\\n    all_results['SVM_f1_IQR'] = anomaly_1clssvm_IQR['f1_IQR']\\\\\\\\n  File \\\\\\\\\\\\\\\"/azureml-envs/azureml_2985c9e26d721291a108505866e36889/lib/python3.6/site-packages/pandas/core/frame.py\\\\\\\\\\\\\\\", line 2906, in __getitem__\\\\\\\\n    indexer = self.columns.get_loc(key)\\\\\\\\n  File \\\\\\\\\\\\\\\"/azureml-envs/azureml_2985c9e26d721291a108505866e36889/lib/python3.6/site-packages/pandas/core/indexes/base.py\\\\\\\\\\\\\\\", line 2900, in get_loc\\\\\\\\n    raise KeyError(key) from err\\\\\\\\nKeyError: 'f1_IQR'\\\\\\\\n\\\\\\\\n'. Please check the log file 'user_logs/std_log.txt' for more details.\\\\\\\"}\\\\n{\\\\n  \\\\\\\"code\\\\\\\": \\\\\\\"ExecutionFailed\\\\\\\",\\\\n  \\\\\\\"target\\\\\\\": \\\\\\\"\\\\\\\",\\\\n  \\\\\\\"category\\\\\\\": \\\\\\\"UserError\\\\\\\",\\\\n  \\\\\\\"error_details\\\\\\\": [\\\\n    {\\\\n      \\\\\\\"key\\\\\\\": \\\\\\\"exit_codes\\\\\\\",\\\\n      \\\\\\\"value\\\\\\\": \\\\\\\"1\\\\\\\"\\\\n    }\\\\n  ]\\\\n}\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1659961476165
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}